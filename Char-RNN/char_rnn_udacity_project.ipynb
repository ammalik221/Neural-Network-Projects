{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "char-rnn-udacity-project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_z6wfWfQaL9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importing libraries\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhlkQGsPQi_q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load data\n",
        "\n",
        "with open('drive/My Drive/style_transfer/input.txt', 'r') as f:\n",
        "    text = f.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ON54DNviT8kL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "7d426d9a-88e0-490b-c580-9c741ed88d8a"
      },
      "source": [
        "# infor about the data\n",
        "print(text[:50])\n",
        "print(\"type = \", type(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear\n",
            "type =  <class 'str'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aM-LlVaQnZn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pre-processing data\n",
        "\n",
        "\n",
        "# one-hot encoding\n",
        "def one_hot_encode(arr, n_labels):\n",
        "    \n",
        "    # we will make a new array which is one-hot encoded\n",
        "    # that is for every example the array will consist of classes encoded as 0 or 1\n",
        "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
        "    \n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "    \n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "    \n",
        "    return one_hot\n",
        "\n",
        "def get_batches(arr, batch_size, seq_length):\n",
        "    \"\"\"\n",
        "    The input to the model will be in the form of batches, where each \n",
        "    batch is of a fixed length.\n",
        "    We are gonna ignore the characters which will be left after dividing the\n",
        "    text into batches.\n",
        "    \"\"\"\n",
        "    \n",
        "    # total batches\n",
        "    batch_size_total = batch_size * seq_length\n",
        "    n_batches = len(arr)//batch_size_total\n",
        "    \n",
        "    arr = arr[:n_batches * batch_size_total]\n",
        "    arr = arr.reshape((batch_size, -1))\n",
        "    \n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        # The features\n",
        "        x = arr[:, n:n+seq_length]\n",
        "        # The targets, shifted by one\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RODKOprR3MWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creates a tuple of unique characters from the text\n",
        "unique_characters = tuple(set(text))\n",
        "\n",
        "# assigning a integer value to each character\n",
        "int2char = dict(enumerate(unique_characters))\n",
        "char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "\n",
        "# replace each character with their corresponding integer value\n",
        "encoded = np.array([char2int[ch] for ch in text])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIdLyZzB3NU-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "1b7a7283-8436-4e47-c1ef-de37b25a4159"
      },
      "source": [
        "print(\"unique characters = \", unique_characters, '\\n')\n",
        "print(\"Number of unique characters = \", len(unique_characters), '\\n')\n",
        "\n",
        "print(\"int2char = \", int2char, '\\n')\n",
        "\n",
        "print(\"char2int = \", char2int, '\\n')\n",
        "\n",
        "print(\"encoded text = \", encoded[:50], '\\n')\n",
        "print(\"input shape = \", encoded.shape, '\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unique characters =  ('X', 'U', '$', 'N', 'j', 'C', 'F', 'b', 'W', 'd', 'R', 'p', 'c', '.', 'S', 'Y', 'Q', 'y', 'l', 's', ' ', 'B', ',', 'O', 'V', 'f', 'A', 'T', 'I', 'P', 'G', 'Z', 'x', 'J', 'g', '&', 'r', 'K', '\\n', '-', 'a', 't', 'z', 'u', 'D', \"'\", 'v', 'h', 'q', '3', 'w', 'm', 'E', 'o', 'n', 'L', 'e', '!', 'H', '?', 'k', 'M', 'i', ';', ':') \n",
            "\n",
            "Number of unique characters =  65 \n",
            "\n",
            "int2char =  {0: 'X', 1: 'U', 2: '$', 3: 'N', 4: 'j', 5: 'C', 6: 'F', 7: 'b', 8: 'W', 9: 'd', 10: 'R', 11: 'p', 12: 'c', 13: '.', 14: 'S', 15: 'Y', 16: 'Q', 17: 'y', 18: 'l', 19: 's', 20: ' ', 21: 'B', 22: ',', 23: 'O', 24: 'V', 25: 'f', 26: 'A', 27: 'T', 28: 'I', 29: 'P', 30: 'G', 31: 'Z', 32: 'x', 33: 'J', 34: 'g', 35: '&', 36: 'r', 37: 'K', 38: '\\n', 39: '-', 40: 'a', 41: 't', 42: 'z', 43: 'u', 44: 'D', 45: \"'\", 46: 'v', 47: 'h', 48: 'q', 49: '3', 50: 'w', 51: 'm', 52: 'E', 53: 'o', 54: 'n', 55: 'L', 56: 'e', 57: '!', 58: 'H', 59: '?', 60: 'k', 61: 'M', 62: 'i', 63: ';', 64: ':'} \n",
            "\n",
            "char2int =  {'X': 0, 'U': 1, '$': 2, 'N': 3, 'j': 4, 'C': 5, 'F': 6, 'b': 7, 'W': 8, 'd': 9, 'R': 10, 'p': 11, 'c': 12, '.': 13, 'S': 14, 'Y': 15, 'Q': 16, 'y': 17, 'l': 18, 's': 19, ' ': 20, 'B': 21, ',': 22, 'O': 23, 'V': 24, 'f': 25, 'A': 26, 'T': 27, 'I': 28, 'P': 29, 'G': 30, 'Z': 31, 'x': 32, 'J': 33, 'g': 34, '&': 35, 'r': 36, 'K': 37, '\\n': 38, '-': 39, 'a': 40, 't': 41, 'z': 42, 'u': 43, 'D': 44, \"'\": 45, 'v': 46, 'h': 47, 'q': 48, '3': 49, 'w': 50, 'm': 51, 'E': 52, 'o': 53, 'n': 54, 'L': 55, 'e': 56, '!': 57, 'H': 58, '?': 59, 'k': 60, 'M': 61, 'i': 62, ';': 63, ':': 64} \n",
            "\n",
            "encoded text =  [ 6 62 36 19 41 20  5 62 41 62 42 56 54 64 38 21 56 25 53 36 56 20 50 56\n",
            " 20 11 36 53 12 56 56  9 20 40 54 17 20 25 43 36 41 47 56 36 22 20 47 56\n",
            " 40 36] \n",
            "\n",
            "input shape =  (1115394,) \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsq8GnW_Rewa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4924ad4b-98a1-43a0-8d79-59f49f45ee67"
      },
      "source": [
        "# training\n",
        "\n",
        "# check for gpu\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU!')\n",
        "else: \n",
        "    print('Training on CPU')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPTg9Z86TG_N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model\n",
        "\n",
        "class CharRNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2, drop_prob=0.5, lr=0.001):\n",
        "\n",
        "        \"\"\" initialising the layers of the network\"\"\"\n",
        "        super().__init__()\n",
        "        # dropout layer\n",
        "        self.drop_prob = drop_prob\n",
        "        # number of layers\n",
        "        self.n_layers = n_layers\n",
        "        # number of nodes in hidden layer \n",
        "        self.n_hidden = n_hidden\n",
        "        # learning rate \n",
        "        self.lr = lr\n",
        "        \n",
        "        # unique characters in our word dictionary\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "        \n",
        "        # lstm layer\n",
        "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "      \n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "        \n",
        "        # output, hidden = lstm(input, initial_hidden)\n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "        \n",
        "        out = self.dropout(r_output)\n",
        "        \n",
        "        # using contiguous to reshape the output \n",
        "        # to match it the fc layer\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "        \n",
        "        out = self.fc(out)\n",
        "        \n",
        "        return out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        \"\"\"\n",
        "        hidden is a tuple (hidden_state, cell_state)\n",
        "        Currently initialising them to zero.\n",
        "        \"\"\"\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "        \n",
        "        return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jfwZvjl6njx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "5725816b-0814-4341-c9c3-7daab6fc7d5c"
      },
      "source": [
        "n_hidden=512\n",
        "n_layers=2\n",
        "\n",
        "net = CharRNN(unique_characters, n_hidden, n_layers)\n",
        "print(net)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(65, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=65, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__htJ8pHTRcf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training\n",
        "\n",
        "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "\n",
        "    # dropout included\n",
        "    net.train()\n",
        "    \n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # splitting of data into train and validation\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "    # move the network to GPU if available\n",
        "    if (train_on_gpu):\n",
        "        net.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden layer\n",
        "        h = net.init_hidden(batch_size)\n",
        "        \n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "            \n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "            if(train_on_gpu):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # hidden layer tuple\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero out the gradients\n",
        "            net.zero_grad()\n",
        "            \n",
        "            output, h = net(inputs, h)\n",
        "            \n",
        "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "            loss.backward()\n",
        "\n",
        "            # clipping the gradient\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "            \n",
        "            if counter % print_every == 0:\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                    \n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    \n",
        "                    inputs, targets = x, y\n",
        "                    if(train_on_gpu):\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "                \n",
        "                    val_losses.append(val_loss.item())\n",
        "                \n",
        "                net.train()\n",
        "                \n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ke6SHJ7AThd3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a3b92080-1354-47f0-b9ae-eb8b534c9cce"
      },
      "source": [
        "batch_size = 128\n",
        "seq_length = 100\n",
        "n_epochs = 20\n",
        "\n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/20... Step: 10... Loss: 3.4156... Val Loss: 3.4125\n",
            "Epoch: 1/20... Step: 20... Loss: 3.3230... Val Loss: 3.3606\n",
            "Epoch: 1/20... Step: 30... Loss: 3.3451... Val Loss: 3.3460\n",
            "Epoch: 1/20... Step: 40... Loss: 3.3562... Val Loss: 3.3408\n",
            "Epoch: 1/20... Step: 50... Loss: 3.3366... Val Loss: 3.3393\n",
            "Epoch: 1/20... Step: 60... Loss: 3.3265... Val Loss: 3.3368\n",
            "Epoch: 1/20... Step: 70... Loss: 3.3017... Val Loss: 3.3327\n",
            "Epoch: 2/20... Step: 80... Loss: 3.3026... Val Loss: 3.3166\n",
            "Epoch: 2/20... Step: 90... Loss: 3.2995... Val Loss: 3.3134\n",
            "Epoch: 2/20... Step: 100... Loss: 3.2698... Val Loss: 3.2735\n",
            "Epoch: 2/20... Step: 110... Loss: 3.1991... Val Loss: 3.1668\n",
            "Epoch: 2/20... Step: 120... Loss: 3.0787... Val Loss: 3.0362\n",
            "Epoch: 2/20... Step: 130... Loss: 2.9640... Val Loss: 2.9159\n",
            "Epoch: 2/20... Step: 140... Loss: 2.8795... Val Loss: 2.8224\n",
            "Epoch: 2/20... Step: 150... Loss: 2.7676... Val Loss: 2.7112\n",
            "Epoch: 3/20... Step: 160... Loss: 2.6843... Val Loss: 2.6276\n",
            "Epoch: 3/20... Step: 170... Loss: 2.6104... Val Loss: 2.5486\n",
            "Epoch: 3/20... Step: 180... Loss: 2.5374... Val Loss: 2.4966\n",
            "Epoch: 3/20... Step: 190... Loss: 2.5201... Val Loss: 2.4464\n",
            "Epoch: 3/20... Step: 200... Loss: 2.4578... Val Loss: 2.3945\n",
            "Epoch: 3/20... Step: 210... Loss: 2.4117... Val Loss: 2.3676\n",
            "Epoch: 3/20... Step: 220... Loss: 2.3732... Val Loss: 2.3482\n",
            "Epoch: 3/20... Step: 230... Loss: 2.3532... Val Loss: 2.3067\n",
            "Epoch: 4/20... Step: 240... Loss: 2.3239... Val Loss: 2.2715\n",
            "Epoch: 4/20... Step: 250... Loss: 2.3075... Val Loss: 2.2556\n",
            "Epoch: 4/20... Step: 260... Loss: 2.2638... Val Loss: 2.2302\n",
            "Epoch: 4/20... Step: 270... Loss: 2.2718... Val Loss: 2.2070\n",
            "Epoch: 4/20... Step: 280... Loss: 2.2456... Val Loss: 2.1944\n",
            "Epoch: 4/20... Step: 290... Loss: 2.2191... Val Loss: 2.1713\n",
            "Epoch: 4/20... Step: 300... Loss: 2.1864... Val Loss: 2.1505\n",
            "Epoch: 4/20... Step: 310... Loss: 2.1428... Val Loss: 2.1288\n",
            "Epoch: 5/20... Step: 320... Loss: 2.1560... Val Loss: 2.1089\n",
            "Epoch: 5/20... Step: 330... Loss: 2.1403... Val Loss: 2.0999\n",
            "Epoch: 5/20... Step: 340... Loss: 2.1156... Val Loss: 2.0876\n",
            "Epoch: 5/20... Step: 350... Loss: 2.1103... Val Loss: 2.0714\n",
            "Epoch: 5/20... Step: 360... Loss: 2.0856... Val Loss: 2.0687\n",
            "Epoch: 5/20... Step: 370... Loss: 2.0680... Val Loss: 2.0498\n",
            "Epoch: 5/20... Step: 380... Loss: 2.0581... Val Loss: 2.0396\n",
            "Epoch: 5/20... Step: 390... Loss: 2.0538... Val Loss: 2.0215\n",
            "Epoch: 6/20... Step: 400... Loss: 2.0378... Val Loss: 2.0068\n",
            "Epoch: 6/20... Step: 410... Loss: 2.0046... Val Loss: 2.0015\n",
            "Epoch: 6/20... Step: 420... Loss: 1.9526... Val Loss: 1.9954\n",
            "Epoch: 6/20... Step: 430... Loss: 1.9771... Val Loss: 1.9903\n",
            "Epoch: 6/20... Step: 440... Loss: 1.9565... Val Loss: 1.9736\n",
            "Epoch: 6/20... Step: 450... Loss: 1.9839... Val Loss: 1.9648\n",
            "Epoch: 6/20... Step: 460... Loss: 1.9363... Val Loss: 1.9509\n",
            "Epoch: 7/20... Step: 470... Loss: 1.9545... Val Loss: 1.9383\n",
            "Epoch: 7/20... Step: 480... Loss: 1.9336... Val Loss: 1.9301\n",
            "Epoch: 7/20... Step: 490... Loss: 1.9240... Val Loss: 1.9269\n",
            "Epoch: 7/20... Step: 500... Loss: 1.9243... Val Loss: 1.9177\n",
            "Epoch: 7/20... Step: 510... Loss: 1.9144... Val Loss: 1.9104\n",
            "Epoch: 7/20... Step: 520... Loss: 1.9116... Val Loss: 1.9041\n",
            "Epoch: 7/20... Step: 530... Loss: 1.8976... Val Loss: 1.8956\n",
            "Epoch: 7/20... Step: 540... Loss: 1.8995... Val Loss: 1.8960\n",
            "Epoch: 8/20... Step: 550... Loss: 1.8916... Val Loss: 1.8804\n",
            "Epoch: 8/20... Step: 560... Loss: 1.8742... Val Loss: 1.8761\n",
            "Epoch: 8/20... Step: 570... Loss: 1.8334... Val Loss: 1.8695\n",
            "Epoch: 8/20... Step: 580... Loss: 1.8788... Val Loss: 1.8589\n",
            "Epoch: 8/20... Step: 590... Loss: 1.8490... Val Loss: 1.8546\n",
            "Epoch: 8/20... Step: 600... Loss: 1.8247... Val Loss: 1.8444\n",
            "Epoch: 8/20... Step: 610... Loss: 1.8105... Val Loss: 1.8410\n",
            "Epoch: 8/20... Step: 620... Loss: 1.8209... Val Loss: 1.8327\n",
            "Epoch: 9/20... Step: 630... Loss: 1.8261... Val Loss: 1.8257\n",
            "Epoch: 9/20... Step: 640... Loss: 1.7964... Val Loss: 1.8281\n",
            "Epoch: 9/20... Step: 650... Loss: 1.8027... Val Loss: 1.8187\n",
            "Epoch: 9/20... Step: 660... Loss: 1.7943... Val Loss: 1.8104\n",
            "Epoch: 9/20... Step: 670... Loss: 1.7956... Val Loss: 1.8090\n",
            "Epoch: 9/20... Step: 680... Loss: 1.7896... Val Loss: 1.8012\n",
            "Epoch: 9/20... Step: 690... Loss: 1.7821... Val Loss: 1.7967\n",
            "Epoch: 9/20... Step: 700... Loss: 1.7450... Val Loss: 1.7904\n",
            "Epoch: 10/20... Step: 710... Loss: 1.7853... Val Loss: 1.7821\n",
            "Epoch: 10/20... Step: 720... Loss: 1.7608... Val Loss: 1.7807\n",
            "Epoch: 10/20... Step: 730... Loss: 1.7366... Val Loss: 1.7762\n",
            "Epoch: 10/20... Step: 740... Loss: 1.7285... Val Loss: 1.7700\n",
            "Epoch: 10/20... Step: 750... Loss: 1.7427... Val Loss: 1.7665\n",
            "Epoch: 10/20... Step: 760... Loss: 1.7328... Val Loss: 1.7685\n",
            "Epoch: 10/20... Step: 770... Loss: 1.7184... Val Loss: 1.7607\n",
            "Epoch: 10/20... Step: 780... Loss: 1.7245... Val Loss: 1.7468\n",
            "Epoch: 11/20... Step: 790... Loss: 1.7339... Val Loss: 1.7436\n",
            "Epoch: 11/20... Step: 800... Loss: 1.7090... Val Loss: 1.7438\n",
            "Epoch: 11/20... Step: 810... Loss: 1.6468... Val Loss: 1.7428\n",
            "Epoch: 11/20... Step: 820... Loss: 1.6773... Val Loss: 1.7389\n",
            "Epoch: 11/20... Step: 830... Loss: 1.6769... Val Loss: 1.7374\n",
            "Epoch: 11/20... Step: 840... Loss: 1.7103... Val Loss: 1.7347\n",
            "Epoch: 11/20... Step: 850... Loss: 1.6887... Val Loss: 1.7291\n",
            "Epoch: 12/20... Step: 860... Loss: 1.6941... Val Loss: 1.7145\n",
            "Epoch: 12/20... Step: 870... Loss: 1.6700... Val Loss: 1.7129\n",
            "Epoch: 12/20... Step: 880... Loss: 1.6497... Val Loss: 1.7129\n",
            "Epoch: 12/20... Step: 890... Loss: 1.6684... Val Loss: 1.7115\n",
            "Epoch: 12/20... Step: 900... Loss: 1.6516... Val Loss: 1.7097\n",
            "Epoch: 12/20... Step: 910... Loss: 1.6770... Val Loss: 1.7060\n",
            "Epoch: 12/20... Step: 920... Loss: 1.6827... Val Loss: 1.7029\n",
            "Epoch: 12/20... Step: 930... Loss: 1.6809... Val Loss: 1.6948\n",
            "Epoch: 13/20... Step: 940... Loss: 1.6802... Val Loss: 1.6876\n",
            "Epoch: 13/20... Step: 950... Loss: 1.6552... Val Loss: 1.6904\n",
            "Epoch: 13/20... Step: 960... Loss: 1.6198... Val Loss: 1.6874\n",
            "Epoch: 13/20... Step: 970... Loss: 1.6715... Val Loss: 1.6861\n",
            "Epoch: 13/20... Step: 980... Loss: 1.6485... Val Loss: 1.6883\n",
            "Epoch: 13/20... Step: 990... Loss: 1.6275... Val Loss: 1.6835\n",
            "Epoch: 13/20... Step: 1000... Loss: 1.6185... Val Loss: 1.6793\n",
            "Epoch: 13/20... Step: 1010... Loss: 1.6265... Val Loss: 1.6733\n",
            "Epoch: 14/20... Step: 1020... Loss: 1.6355... Val Loss: 1.6653\n",
            "Epoch: 14/20... Step: 1030... Loss: 1.6071... Val Loss: 1.6708\n",
            "Epoch: 14/20... Step: 1040... Loss: 1.6232... Val Loss: 1.6698\n",
            "Epoch: 14/20... Step: 1050... Loss: 1.6088... Val Loss: 1.6687\n",
            "Epoch: 14/20... Step: 1060... Loss: 1.6262... Val Loss: 1.6667\n",
            "Epoch: 14/20... Step: 1070... Loss: 1.6168... Val Loss: 1.6631\n",
            "Epoch: 14/20... Step: 1080... Loss: 1.6064... Val Loss: 1.6571\n",
            "Epoch: 14/20... Step: 1090... Loss: 1.5869... Val Loss: 1.6478\n",
            "Epoch: 15/20... Step: 1100... Loss: 1.6250... Val Loss: 1.6437\n",
            "Epoch: 15/20... Step: 1110... Loss: 1.5970... Val Loss: 1.6477\n",
            "Epoch: 15/20... Step: 1120... Loss: 1.5812... Val Loss: 1.6494\n",
            "Epoch: 15/20... Step: 1130... Loss: 1.5548... Val Loss: 1.6489\n",
            "Epoch: 15/20... Step: 1140... Loss: 1.5858... Val Loss: 1.6424\n",
            "Epoch: 15/20... Step: 1150... Loss: 1.5726... Val Loss: 1.6404\n",
            "Epoch: 15/20... Step: 1160... Loss: 1.5701... Val Loss: 1.6375\n",
            "Epoch: 15/20... Step: 1170... Loss: 1.5767... Val Loss: 1.6270\n",
            "Epoch: 16/20... Step: 1180... Loss: 1.5889... Val Loss: 1.6296\n",
            "Epoch: 16/20... Step: 1190... Loss: 1.5815... Val Loss: 1.6309\n",
            "Epoch: 16/20... Step: 1200... Loss: 1.5202... Val Loss: 1.6336\n",
            "Epoch: 16/20... Step: 1210... Loss: 1.5310... Val Loss: 1.6287\n",
            "Epoch: 16/20... Step: 1220... Loss: 1.5361... Val Loss: 1.6275\n",
            "Epoch: 16/20... Step: 1230... Loss: 1.5780... Val Loss: 1.6255\n",
            "Epoch: 16/20... Step: 1240... Loss: 1.5619... Val Loss: 1.6212\n",
            "Epoch: 17/20... Step: 1250... Loss: 1.5624... Val Loss: 1.6108\n",
            "Epoch: 17/20... Step: 1260... Loss: 1.5435... Val Loss: 1.6147\n",
            "Epoch: 17/20... Step: 1270... Loss: 1.5293... Val Loss: 1.6223\n",
            "Epoch: 17/20... Step: 1280... Loss: 1.5448... Val Loss: 1.6144\n",
            "Epoch: 17/20... Step: 1290... Loss: 1.5353... Val Loss: 1.6134\n",
            "Epoch: 17/20... Step: 1300... Loss: 1.5489... Val Loss: 1.6105\n",
            "Epoch: 17/20... Step: 1310... Loss: 1.5549... Val Loss: 1.6072\n",
            "Epoch: 17/20... Step: 1320... Loss: 1.5573... Val Loss: 1.6035\n",
            "Epoch: 18/20... Step: 1330... Loss: 1.5683... Val Loss: 1.5973\n",
            "Epoch: 18/20... Step: 1340... Loss: 1.5302... Val Loss: 1.5986\n",
            "Epoch: 18/20... Step: 1350... Loss: 1.5099... Val Loss: 1.6000\n",
            "Epoch: 18/20... Step: 1360... Loss: 1.5496... Val Loss: 1.6066\n",
            "Epoch: 18/20... Step: 1370... Loss: 1.5445... Val Loss: 1.6020\n",
            "Epoch: 18/20... Step: 1380... Loss: 1.5240... Val Loss: 1.5979\n",
            "Epoch: 18/20... Step: 1390... Loss: 1.5173... Val Loss: 1.5981\n",
            "Epoch: 18/20... Step: 1400... Loss: 1.5278... Val Loss: 1.5905\n",
            "Epoch: 19/20... Step: 1410... Loss: 1.5281... Val Loss: 1.5840\n",
            "Epoch: 19/20... Step: 1420... Loss: 1.4931... Val Loss: 1.5865\n",
            "Epoch: 19/20... Step: 1430... Loss: 1.5094... Val Loss: 1.5884\n",
            "Epoch: 19/20... Step: 1440... Loss: 1.5030... Val Loss: 1.5880\n",
            "Epoch: 19/20... Step: 1450... Loss: 1.5159... Val Loss: 1.5910\n",
            "Epoch: 19/20... Step: 1460... Loss: 1.5271... Val Loss: 1.5801\n",
            "Epoch: 19/20... Step: 1470... Loss: 1.5081... Val Loss: 1.5798\n",
            "Epoch: 19/20... Step: 1480... Loss: 1.4845... Val Loss: 1.5753\n",
            "Epoch: 20/20... Step: 1490... Loss: 1.5351... Val Loss: 1.5745\n",
            "Epoch: 20/20... Step: 1500... Loss: 1.4995... Val Loss: 1.5744\n",
            "Epoch: 20/20... Step: 1510... Loss: 1.4738... Val Loss: 1.5763\n",
            "Epoch: 20/20... Step: 1520... Loss: 1.4673... Val Loss: 1.5786\n",
            "Epoch: 20/20... Step: 1530... Loss: 1.4857... Val Loss: 1.5745\n",
            "Epoch: 20/20... Step: 1540... Loss: 1.4891... Val Loss: 1.5755\n",
            "Epoch: 20/20... Step: 1550... Loss: 1.4826... Val Loss: 1.5714\n",
            "Epoch: 20/20... Step: 1560... Loss: 1.4838... Val Loss: 1.5657\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "066Ye7G_TktE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(net, char, h=None, top_k=None): \n",
        "    \"\"\"\n",
        "    Takes in an input and predicts a new character.\n",
        "    \"\"\"\n",
        "    x = np.array([[net.char2int[char]]])\n",
        "    x = one_hot_encode(x, len(net.chars))\n",
        "    inputs = torch.from_numpy(x)\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        inputs = inputs.cuda()\n",
        "\n",
        "    h = tuple([each.data for each in h])\n",
        "    out, h = net(inputs, h)\n",
        "\n",
        "    p = F.softmax(out, dim=1).data\n",
        "    if(train_on_gpu):\n",
        "        p = p.cpu()\n",
        "\n",
        "    if top_k is None:\n",
        "        top_ch = np.arange(len(net.chars))\n",
        "    else:\n",
        "        p, top_ch = p.topk(top_k)\n",
        "        top_ch = top_ch.numpy().squeeze()\n",
        "\n",
        "    p = p.numpy().squeeze()\n",
        "    char = np.random.choice(top_ch, p=p/p.sum())\n",
        "\n",
        "    return net.int2char[char], h\n",
        "\n",
        "\n",
        "def sample(net, size, prime='The', top_k=None):\n",
        "        \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "    \n",
        "    net.eval()\n",
        "    \n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "    \n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvooDwrGTs9P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "b5e2dd53-cdee-4e3f-fe8b-2b47081b3274"
      },
      "source": [
        "print(sample(net, 1000, prime='Anna', top_k=5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Annart in his coutizents.\n",
            "\n",
            "SAMPSSN:\n",
            "Why? his night, to the muntre so by a man and sigh,\n",
            "That he, all, the crouns a sent at and served\n",
            "In another for the seast will both to him\n",
            "The plaster and true to the bark will strike\n",
            "That many a sea in so to make and word.\n",
            "\n",
            "GLOUCESTER:\n",
            "Now what, and to be mothed is the son,\n",
            "Which taken me that with the down time, and see\n",
            "The bang and shall be therefore wass he ware.\n",
            "\n",
            "LARTIUS:\n",
            "I have bathine, with thy seat,\n",
            "A precares words:--if you are a subjects\n",
            "Will but he would had to be pastod and\n",
            "And sell these thanks, and so thou have ston to see,\n",
            "And so my son, and boy a such me breath\n",
            "That we well stand to the side of this land;\n",
            "And she shall seath a musines of all.\n",
            "\n",
            "GLOUCESTER:\n",
            "I am a state there of thy craisor,\n",
            "This shall be strongs; but we shall spain thou wert.\n",
            "\n",
            "ROMEO:\n",
            "Hark me the cretting that who thou art now here\n",
            "At the poor poor straight and have being here\n",
            "By hang in strength to thee; fear to-time\n",
            "With them or him with his, which is her last;\n",
            "The cass o\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iZj9O04HLWw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}